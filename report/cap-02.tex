%Demonstra como foi realizado o trabalho
%• Pesquisas bibliográficas: sempre com as referências
%• Modelagem
%• Sistema gerado
%• Testes de desempenho/avaliação
\chapter{Métodos}
\noindent
\section{Bag of Words}
%https://www3.nd.edu/~dchiang/teaching/nlp/2015/notes/chapter1v2.pdf
%https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a
Bag of words não se trata de modelo de classificação por completo, mas sim de uma estratégia de representação do texto que a sequência de palavras é desconsiderada. Dessa maneira, o texto é simplificado para uma lista de palavras distintas e a respectiva contagem de ocorrência de cada uma delas.
We normally view text as a sequence of words, or we can impose additional structure on it like syntax. Or,
we can dissolve some structure, namely, the order of the words. This gives a bag of words, which simply
counts how many times each word occurs in a sentence (or document).
%the cat sat on the mat → {cat,mat,on,sat,the,the}
Why? First, this representation is computationally extremely easy to work with. Second, this representation
melts a text down into bits of meaning and serves as a crude way of capturing what the text is
“about.” Crude, but often effective.
Tokenization We assume that all the sentences/documents have been tokenized so that the word boundaries
are unambiguous. A commonly used English tokenizer is part of Stanford CoreNLP,1 and LDC has a
simple rule-based tokenizer.2 Both are implemented/wrapped in Python by NLTK.3
Stemming It is also common in bag-of-word approaches to do morphological stemming, that is, removing
affixes like -ed, -ing, etc. A classic stemmer for English is the Porter stemmer,4 and another is the
Lancaster (Paice/Husk) stemmer.5 Both are implemented/wrapped in Python by NLTK.6
Stop Words Finally, it’s also common to remove high-frequency but low-content words

\section{Redes Neurais}
https://www.coursera.org/specializations/deep-learning
https://www.coursera.org/learn/machine-learning
\subsection{Word Embedings}
\subsection{Redes Neurais Convolucionais}
http://cs231n.github.io/
http://cs224d.stanford.edu/syllabus.html
https://www.coursera.org/learn/convolutional-neural-networks
\subsection{Redes Neurais Recorrentes}
http://cs224d.stanford.edu/syllabus.html